# shraddha-zagade-final-task
# üìò Text Processing Toolkit

## üìå Project Overview

This project contains three simple Python utilities for cleaning and
processing text and JSON data.\
It is useful for beginners learning **text preprocessing, data cleaning,
and basic NLP tasks**.

### Tools Included

-   Text Cleaner
-   JSON Preprocessor
-   Tokenizer

------------------------------------------------------------------------

## ‚öôÔ∏è Requirements

-   Python 3.x
-   Only built-in libraries (no external packages)

------------------------------------------------------------------------

## üìÇ Project Files

### 1. text_cleaner.py

Cleans and formats text: - Removes extra spaces - Removes special
characters - Converts text to lowercase

### 2. json_preprocessor.py

Cleans JSON-like data: - Normalizes dictionary keys - Converts keys to
lowercase - Replaces spaces/dashes with underscores - Removes `None`
values

### 3. tokenizer.py

Analyzes text: - Splits text into words - Counts words - Creates word
frequency map

------------------------------------------------------------------------

# ‚ñ∂Ô∏è How to Run the Programs

Open terminal inside the project folder and run:

``` bash
python text_cleaner.py.py
python json_preprocessor.py.py
python tokenizer.py.py
```

------------------------------------------------------------------------

# üßπ Text Cleaner

## Methods

-   `remove_extra_space(text)` ‚Üí removes spaces
-   `remove_special_characters(text)` ‚Üí keeps only letters & numbers
-   `normalize_case()` ‚Üí converts to lowercase

## Example

Input:

    Hello  World!! Welcome@123

Output:

    helloworldwelcome123

## Usage

``` python
from text_cleaner import text_cleaner

cleaner = text_cleaner()
cleaner.remove_extra_space(text)
cleaner.remove_special_characters(text)
cleaner.normalize_case()
```

------------------------------------------------------------------------

# üóÇÔ∏è JSON Preprocessor

## Methods

-   `normalize_keys(json_obj)` ‚Üí cleans keys
-   `strip_nulls(json_obj)` ‚Üí removes None values

## Example

Input:

``` python
{
 " User Name ": "Alice",
 "Age": None
}
```

Output:

``` python
{'user_name': 'Alice'}
```

## Usage

``` python
from json_preprocessor import JsonPreprocessor

processor = JsonPreprocessor()
result = processor.strip_nulls(processor.normalize_keys(data))
print(result)
```

------------------------------------------------------------------------

# üî§ Tokenizer

## Methods

-   `tokenize(text)` ‚Üí split into words
-   `word_count(text)` ‚Üí count words
-   `frequency_map(text)` ‚Üí count occurrences

## Example

Input:

    hello world hello

Output:

    Tokenize: ['hello', 'world', 'hello']
    Word Count: 3
    Frequency Map: {'hello': 2, 'world': 1}

## Usage

``` python
from tokenizer import TextTools

tools = TextTools()
print(tools.tokenize(text))
print(tools.word_count(text))
print(tools.frequency_map(text))
```

------------------------------------------------------------------------

# üéØ Use Cases

-   Cleaning user input
-   Preparing API/JSON data
-   Text analysis
-   Beginner NLP practice
-   Academic mini-projects

------------------------------------------------------------------------

# üöÄ Future Improvements

-   File input/output support
-   Better punctuation handling
-   Stop-word removal
-   Combine all tools into one package

------------------------------------------------------------------------

## üë©‚Äçüíª Author

Created for learning and practice in Python text processing.
